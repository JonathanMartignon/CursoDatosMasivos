{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3c_minhash.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNr0FsYvlJAJT+TKZ8ir2E9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/blancavazquez/CursoDatosMasivos/blob/master/notebooks/3c_minhash.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kloh2BluGySD",
        "colab_type": "text"
      },
      "source": [
        "# Búsqueda de documentos con MinHash\n",
        "En esta libreta veremos cómo hacer búsqueda eficiente de documentos similares considerando la similitud de Jaccard usando MinHash.\n",
        "\n",
        "Primero cargamos los módulos necesarios."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOzaNGg6PA6_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from math import floor \n",
        "\n",
        "import random\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_56-act4ikel",
        "colab_type": "text"
      },
      "source": [
        "Vamos a usar el conjunto de documentos de _20 Newsgropus_, el cual descargamos usando scikit-learn. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7RLivGmPgIQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "db = fetch_20newsgroups(remove=('headers','footers','quotes'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Alui_Ua7iva9",
        "colab_type": "text"
      },
      "source": [
        "Definimos nuestro analizador léxico usando la biblioteca NLTK. Vamos a extraer los componentes léxicos, pasarlos a minúsculas y lematizarlos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnA4Yn-HPLQ7",
        "colab_type": "code",
        "outputId": "ddd84660-f2b8-4de5-8e08-6070c80b0dee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download(['punkt','averaged_perceptron_tagger','wordnet'])\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import word_tokenize, pos_tag\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.corpus.reader.wordnet import NOUN, VERB, ADV, ADJ\n",
        "\n",
        "morphy_tag = {\n",
        "    'JJ' : ADJ,\n",
        "    'JJR' : ADJ,\n",
        "    'JJS' : ADJ,\n",
        "    'VB' : VERB,\n",
        "    'VBD' : VERB,\n",
        "    'VBG' : VERB,\n",
        "    'VBN' : VERB,\n",
        "    'VBP' : VERB,\n",
        "    'VBZ' : VERB,\n",
        "    'RB' : ADV,\n",
        "    'RBR' : ADV,\n",
        "    'RBS' : ADV\n",
        "}\n",
        "\n",
        "def doc_a_tokens(doc):\n",
        "  tagged = pos_tag(word_tokenize(doc.lower()))\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  tokens = []\n",
        "  for p,t in tagged:\n",
        "    tokens.append(lemmatizer.lemmatize(p, pos=morphy_tag.get(t, NOUN)))\n",
        "\n",
        "  return tokens"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ado-nD45jFNP",
        "colab_type": "text"
      },
      "source": [
        "Dividimos nuestro conjunto en 2 subconjuntos: los documentos de la base que se buscarán y los documentos de consulta."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cduiS-ukgm7m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n = len(db.data)\n",
        "perm = np.random.permutation(n).astype(int)\n",
        "n_ej = int(floor(n * 0.95))\n",
        "\n",
        "base = [db.data[i] for i in perm[:n_ej]]\n",
        "consultas = [db.data[i] for i in perm[n_ej:]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOpgvsMJjYsf",
        "colab_type": "text"
      },
      "source": [
        "Calculamos las bolsas de palabras del conjunto base usando la clase `CountVectorizer` de scikit-learn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxFG3nyyPOVs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "docs_base = []\n",
        "for d in base:\n",
        "  d = d.replace('\\n',' ').replace('\\r',' ').replace('\\t',' ')\n",
        "  tokens = doc_a_tokens(d)\n",
        "  docs_base.append(' '.join(tokens))\n",
        "v = CountVectorizer(stop_words='english', max_features=5000, max_df=0.8)\n",
        "bolsas_base = v.fit_transform(docs_base)\n",
        "\n",
        "dim = bolsas_base.shape[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtNlXvCgjlE2",
        "colab_type": "text"
      },
      "source": [
        "También calculamos las bolsas para las consultas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtaZjNQuhgs9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "docs_consultas = []\n",
        "for d in consultas:\n",
        "  d = d.replace('\\n',' ').replace('\\r',' ').replace('\\t',' ')\n",
        "  tokens = doc_a_tokens(d)\n",
        "  docs_consultas.append(' '.join(tokens))\n",
        "\n",
        "\n",
        "bolsas_consultas = v.transform(consultas)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ejYh-CijqkP",
        "colab_type": "text"
      },
      "source": [
        "Finalmente, definimos nuestra clase para MinHash, la cual encapsula las funciones para calcular los valores MinHash, las tuplas y los índices, la tabla y las operaciones de inserción, búsqueda y eliminación sobre esta. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQR1PIsnyCPt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MinHashTable:\n",
        "  def __init__(self, n_cubetas, t_tupla, dim):\n",
        "    self.n_cubetas = n_cubetas\n",
        "    self.tabla = [[] for i in range(n_cubetas)]\n",
        "    self.dim = dim\n",
        "    self.t_tupla = t_tupla\n",
        "    self.perm = np.random.randint(0, np.iinfo(np.int32).max, size=(self.dim, self.t_tupla))\n",
        "    self.rind = np.random.randint(0, np.iinfo(np.int32).max, size=(self.dim, self.t_tupla))\n",
        "    self.a = np.random.randint(0, np.iinfo(np.int32).max, size=self.t_tupla)\n",
        "    self.b = np.random.randint(0, np.iinfo(np.int32).max, size=self.t_tupla)\n",
        "    self.primo = 4294967291\n",
        "\n",
        "  def __repr__(self):\n",
        "    contenido = ['%d::%s' % (i, self.tabla[i]) for i in range(self.n_cubetas)]\n",
        "    return \"<TablaHash :%s >\" % ('\\n'.join(contenido))\n",
        "\n",
        "  def __str__(self):\n",
        "    contenido = ['%d::%s' % (i, self.tabla[i]) for i in range(self.n_cubetas) if self.tabla[i]]\n",
        "    return '\\n'.join(contenido)\n",
        "\n",
        "  def sl(self, x, i):\n",
        "    return (self.h(x) + i) % self.n_cubetas\n",
        "\n",
        "  def h(self, x):\n",
        "    return x % self.primo\n",
        "\n",
        "  def minhash(self, x):\n",
        "    xp = self.perm[x]\n",
        "    xi = self.rind[x]\n",
        "    amin = xp.argmin(axis = 0)\n",
        "    \n",
        "    pmin = xp[amin, np.arange(0, self.t_tupla)]\n",
        "    emin = self.rind[x][amin, np.arange(0, self.t_tupla)]\n",
        "    return np.sum(self.a * pmin, dtype=np.ulonglong), np.sum(self.b * emin, dtype=np.ulonglong)\n",
        "     \n",
        "  def insertar(self, x, ident):\n",
        "    v1, v2 = self.minhash(x)\n",
        "    mh = self.h(v1)\n",
        "  \n",
        "    llena = True\n",
        "    for i in range(self.n_cubetas):\n",
        "      cubeta = int(self.sl(v2, i))\n",
        "      if not self.tabla[cubeta]:\n",
        "        self.tabla[cubeta].append(mh)\n",
        "        self.tabla[cubeta].append([ident])\n",
        "        llena = False\n",
        "        break\n",
        "      elif self.tabla[cubeta][0] == mh:\n",
        "        self.tabla[cubeta][1].append(ident)\n",
        "        llena = False\n",
        "        break\n",
        "\n",
        "    if llena:\n",
        "      print('¡Error, tabla llena!')\n",
        "\n",
        "  def buscar(self, x):\n",
        "    v1, v2 = self.minhash(x)\n",
        "    mh = self.h(v1)\n",
        "\n",
        "    for i in range(self.n_cubetas):\n",
        "      cubeta = int(self.sl(v2, i))\n",
        "      if not self.tabla[cubeta]:\n",
        "        return []\n",
        "      elif self.tabla[cubeta][0] == mh:\n",
        "        return self.tabla[cubeta][1]\n",
        "        \n",
        "    return []\n",
        "\n",
        "  def eliminar(self, x, ident):\n",
        "    v1, v2 = self.minhash(x)\n",
        "    mh = self.h(v1)\n",
        "\n",
        "    for i in range(self.n_cubetas):\n",
        "      cubeta = int(self.sl(v2, i))\n",
        "      if not self.tabla[cubeta]:\n",
        "        break\n",
        "      elif self.tabla[cubeta][0] == mh:\n",
        "        return self.tabla[cubeta][1].remove(ident)\n",
        "\n",
        "    return -1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExAaoXELkJzX",
        "colab_type": "text"
      },
      "source": [
        "Ahora instanciamos esta clase tantas veces como tablas queramos para realizar la búsqueda."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7gNvp8yO0nF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_tablas = 10\n",
        "tablas = [MinHashTable(2**21, 3, dim) for _ in range(n_tablas)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNuWULYQkZ2z",
        "colab_type": "text"
      },
      "source": [
        "Definimos una función para convertir de matriz dispersa tipo CSR a una lista de listas. Nota que no se están considerando las frecuencias de las bolsas, por lo que la representación del documento es un conjunto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_lAXGApOspb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def csr_to_ldb(csr):\n",
        "  ldb = [[] for _ in range(csr.shape[0])]\n",
        "  coo = csr.tocoo()    \n",
        "  for i,j,v in zip(coo.row, coo.col, coo.data):\n",
        "    ldb[i].append(j)\n",
        "\n",
        "  return ldb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xL0yfAA8Yyba",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ll_base = csr_to_ldb(bolsas_base)\n",
        "for j,l in enumerate(ll_base):\n",
        "    if l:\n",
        "      for i in range(n_tablas):\n",
        "        tablas[i].insertar(l, j)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylYmsJdqk1AL",
        "colab_type": "text"
      },
      "source": [
        "Recuperamos los documentos similares a nuestros documentos de consulta usando las tablas MinHash."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHkSVxARjwZh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ll_consultas = csr_to_ldb(bolsas_consultas)\n",
        "docs = []\n",
        "for j,l in enumerate(ll_consultas):\n",
        "  dc = []\n",
        "  if l:\n",
        "    for i in range(n_tablas):\n",
        "      dc.extend(tablas[i].buscar(l))\n",
        "  docs.append(set(dc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmyOkNBIlCXV",
        "colab_type": "text"
      },
      "source": [
        "Finalmente, calculamos la similitud Jaccard de los documentos recuperados con los de consulta y los ordenamos por similitud.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDS0K4M6VXT-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def similitud_jaccard(x, y):\n",
        "  x = x.toarray()[0]\n",
        "  y = y.toarray()[0]\n",
        "  inter = np.count_nonzero(x * y)\n",
        "  return inter / (np.count_nonzero(x) + np.count_nonzero(y) - inter)\n",
        "\n",
        "def fuerza_bruta(ds, qs, fd):\n",
        "  medidas = np.zeros(ds.shape[0])\n",
        "  for i,x in enumerate(ds):\n",
        "    medidas[i] = fd(qs, x)\n",
        "\n",
        "  return np.sort(medidas), np.argsort(medidas)\n",
        "\n",
        "sims = []\n",
        "orden = []\n",
        "for i,q in enumerate(bolsas_consultas):\n",
        "  ld = list(docs[i])\n",
        "  if ld:\n",
        "    s,o = fuerza_bruta(bolsas_base[ld], q, similitud_jaccard)\n",
        "    sims.append(s)\n",
        "    orden.append([ld[e] for e in o])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnT6Db27lU-d",
        "colab_type": "text"
      },
      "source": [
        "Examinamos los documentos más similares a uno de los de consulta."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ePYSjvCoH4X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "16de7dfc-77d2-451d-91e6-80b78b7f5ee3"
      },
      "source": [
        "print(consultas[0])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "You are nto alone.  I get the same problems with my Panasonic kpx 1124i (24 \n",
            "pin).  Oterhwise, it's a great printer.  I just can't find a driver for it, \n",
            "only for the non-\"i\" version.  Anyone seen it?\n",
            "\n",
            "Rob\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTUDgDdgob0d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "a6da6e7b-6cb9-4aa6-982a-a3b6d7299e78"
      },
      "source": [
        "print(base[list(docs[0])[0]])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "He is probably referring to the DOS version.. the dos versions is up\n",
            "to like version 6 i think.  The window version just came out recently\n",
            "so it is only up to like version 2 or something.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvxCNshmnMDh",
        "colab_type": "text"
      },
      "source": [
        "## Ejercicio\n",
        "+ Evalúa la búsqueda con distintos valores de $r$ y $\\eta$ usando la fórmula de \n",
        "$$\n",
        "\n",
        "  l = \\frac{log(0.5)}{log(1 - \\eta^r)}\n",
        "\n",
        "$$\n",
        "+ Verifica que las colisiones de los conjuntos aproximan la similitud de Jaccard.\n",
        "+ Extiende la clase `MinHashTable` para que tome en cuenta las multiplicidades de las bolsas."
      ]
    }
  ]
}